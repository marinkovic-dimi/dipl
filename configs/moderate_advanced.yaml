project_name: "klasifikator"
version: "1.0.0"
experiment_name: "moderate_advanced_stratified_dropout"

data:
  raw_data_path: "data/oglasi_za_treniranje.json"
  processed_data_dir: "data/processed"

  text_column: "text"
  class_column: "group_id"
  clean_text_column: "clean_text"

  max_samples: 10000000
  min_samples_per_class: 100
  max_samples_per_class: 1000000
  remove_ostalo_groups: false
  ostalo_groups_file: "data/ostalo-grupe.csv"

  # OPTIMIZED: Increased validation set for stability with 1497 classes
  val_size: 0.15
  test_size: 0.15
  random_state: 42

tokenization:
  vocab_size: 15000
  # OPTIMIZED: Increased max_length to reduce truncation
  max_length: 96
  min_frequency: 2
  special_tokens:
    - "[PAD]"
    - "[CLS]"
    - "[UNK]"
    - "[SEP]"
    - "[MASK]"
  use_cached_tokenizer: true
  tokenizer_cache_dir: "cache/tokenizers"
  # Tokenized dataset caching enabled
  use_tokenized_cache: true
  tokenized_cache_dir: "cache/tokenized_datasets"

model:
  # OPTIMIZED: Balanced complexity
  embedding_dim: 384
  num_heads: 8
  num_layers: 3
  ff_dim: 1024

  # ADVANCED: Stratified dropout with increasing rates through the network
  # Base dropout_rate is used as fallback for any layer that doesn't specify custom rate
  dropout_rate: 0.15

  # Custom dropout rates per layer type:
  embedding_dropout: 0.1      # Lower at input (10%) - preserve input information
  attention_dropout: 0.15     # Moderate (15%) - regularize attention patterns
  ffn_dropout: 0.15           # Moderate (15%) - regularize feed-forward transformations
  dense_dropout: 0.25         # Higher at output (25%) - strong regularization before classification

  # Effective dropout rate calculation (4 layers in series):
  # P(no dropout) = (1-0.1) × (1-0.15) × (1-0.15) × (1-0.25) = 0.488
  # Effective dropout = 1 - 0.488 = 51.2% (well-controlled!)

  pooling_strategy: "attention"  # Better for multi-class classification
  label_smoothing: 0.1           # NOW WORKS after bug fix!

  # OPTIMIZED: Faster learning
  learning_rate: 0.0003
  weight_decay: 0.01
  batch_size: 128
  epochs: 20

  top_k: 5

balancing:
  strategy: "adaptive"

  # OPTIMIZED: More balanced approach
  target_threshold: 3000
  target_threshold_small: 250
  increase_factor: 0.15
  decrease_factor: 0.5
  decrease_factor_small: 0.5

  tier_limits:
    small: 100
    medium: 1000
    large: 5000
    xlarge: 15000
    xxlarge: 40000
    max: 100000

training:
  save_dir: "experiments"
  model_name: "klasifikator_moderate_advanced"
  save_best_only: true
  patience: 7
  monitor: "val_loss"
  mode: "min"

  reduce_lr_factor: 0.5
  reduce_lr_patience: 3
  min_lr: 0.000001

  log_level: "INFO"
  log_to_file: true
  log_dir: "logs"

  checkpoint_freq: "epoch"
  save_frequency: 1

wandb:
  enabled: true
  project: "klasifikator"
  entity: null
  name: "moderate_advanced_stratified_dropout"
  tags:
    - "transformer"
    - "serbian"
    - "advanced-dropout"
    - "stratified-regularization"
  notes: "Advanced config with stratified dropout strategy: 10% embedding, 15% attention/ffn, 25% dense. Effective dropout: 51.2%. Includes label_smoothing fix, reduced complexity, faster LR, larger val set."
  log_model: true
  log_gradients: false
  log_frequency: 100
  save_code: true
